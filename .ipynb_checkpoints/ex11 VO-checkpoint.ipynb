{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 11-12 Visual odometry\n",
    "\n",
    "Visual odometry/ motion estimation from cameras, using the 2D-3D algorithm. Will estimate E-matrices, triangulate 3D landmarks and use PnP to fix poses to positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.1174740e+03, 0.0000000e+00, 1.5019408e+03],\n",
       "       [0.0000000e+00, 3.1174740e+03, 9.8476840e+02],\n",
       "       [0.0000000e+00, 0.0000000e+00, 1.0000000e+00]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the camera matrix\n",
    "K = np.loadtxt('Glyp/K.txt')\n",
    "K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images\n",
    "ext = ['png', 'jpg', 'gif'] # filetypes\n",
    "imdir = \"Glyp/sequence/\"\n",
    "files = []\n",
    "[files.extend(glob.glob(imdir + '*.' + e)) for e in ext]\n",
    "images = [cv2.cvtColor(cv2.imread(file), cv2.COLOR_RGB2GRAY) for file in files]\n",
    "\n",
    "# Look at some images\n",
    "im0, im1, im2 = images[0], images[1], images[2]\n",
    "#fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(14, 4))\n",
    "#axs[0].imshow(im0, cmap=\"gray\")\n",
    "#axs[1].imshow(im1, cmap=\"gray\")\n",
    "#axs[2].imshow(im2, cmap=\"gray\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 11.1 SIFT features and matching\n",
    "\n",
    "Find SIFT keypoints (kp0, kp1, kp2) in all three images and compute their corresponding descriptors (des0, des1, des2).\\\n",
    "(For speed reasons, you can limit the number of SIFT features to 2000.)\n",
    "\n",
    "Convert the features to numpy arrays of 2D points\\\n",
    "```kp = np.array([k.pt for k in kp])```\n",
    "\n",
    "Match the SIFT features between im0 and im1 (matches01), and between im1 and im2 (matches12).\\\n",
    "Convert the matches to numpy arrays of the indices\\\n",
    "```matches = np.array([(m.queryIdx, m.trainIdx) for m in matches]).```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_sift(gray):\n",
    "    \"\"\"\n",
    "    Get stuff\n",
    "    \"\"\"\n",
    "    sift = cv2.xfeatures2d.SIFT_create(nfeatures = 1999) \n",
    "                                \n",
    "    kp, desc = sift.detectAndCompute(gray, None) #Keypoints and descriptors\n",
    "\n",
    "    point = np.array([k.pt for k in kp]).astype(np.int32)\n",
    "    \n",
    "    return point, kp, desc\n",
    "\n",
    "def match_sift(des1, des2, KNN_matcher=False):\n",
    "    \"\"\"\n",
    "    Match stuff\n",
    "    \"\"\"\n",
    "    bf = cv2.BFMatcher() #crossCheck=True\n",
    "    if KNN_matcher:\n",
    "        knn_matches = bf.knnMatch(des1, des2, k=2)\n",
    "        # Ratio test!\n",
    "        ratio_matches = []\n",
    "        for m,n in knn_matches:\n",
    "            if m.distance < 0.75*n.distance:\n",
    "                ratio_matches.append(m)\n",
    "        \n",
    "        # Return the keypoint indexes of matches \n",
    "        return np.array([(m.queryIdx, m.trainIdx) for m in ratio_matches])\n",
    "    else:\n",
    "        all_matches = bf.match(des1, des2)\n",
    "        #sorted_matches = sorted(matches, key = lambda x:x.distance)\n",
    "        return np.array([(m.queryIdx, m.trainIdx) for m in all_matches])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1999, 2), (1999, 2), (1999, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "point0, kp0, des0 = get_all_sift(im0)\n",
    "point1, kp1, des1 = get_all_sift(im1)\n",
    "point2, kp2, des2 = get_all_sift(im2)\n",
    "point0.shape, point1.shape, point2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1999, 2), (1999, 2))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches01 = match_sift(des0, des1) #, KNN_matcher=True) \n",
    "matches12 = match_sift(des1, des2) #, KNN_matcher=True) \n",
    "matches01.shape, matches12.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Look at some images\n",
    "#im0, im1, im2 = images[0], images[1], images[2]\n",
    "#fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(14, 4))\n",
    "#axs[0].imshow(im0, cmap=\"gray\")\n",
    "#axs[0].scatter(point0[:, 0], point0[:, 1], s=2, c=\"r\")\n",
    "#axs[1].imshow(im1, cmap=\"gray\")\n",
    "#axs[1].scatter(point1[:, 0], point1[:, 1], s=2, c=\"r\")\n",
    "#axs[2].imshow(im2, cmap=\"gray\")\n",
    "#axs[2].scatter(point2[:, 0], point2[:, 1], s=2, c=\"r\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EX 11.2 Essential matrix\n",
    "\n",
    "Estimate the essential matrix between im0 and im1 with RANSAC. You can use the OpenCV \\\n",
    "function cv2.findEssentialMat to do this.\n",
    "\n",
    "***NB*** The function expects the points in the correct/ matching order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "563"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E01, Emask01 = cv2.findEssentialMat(point0[matches01[:, 0], :], point1[matches01[:, 1], :], K, method=cv2.RANSAC,\n",
    "                           #prob=0.999, threshold=..., maxIters=...\n",
    "                           )\n",
    "E12, Emask12 = cv2.findEssentialMat(point1[matches12[:, 0], :], point2[matches12[:, 1], :], K, method=cv2.RANSAC,\n",
    "                           #prob=0.999, threshold=..., maxIters=...\n",
    "                           )\n",
    "len(np.where(Emask01>0)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Decompose the essential matrix and find the correct relative pose (R1, t1). For this we can again \\\n",
    "use an OpenCV function namely cv2.recoverPose.\n",
    "- Choose the best of the 4 different options: The one where points are in front of cameras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1418"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#_, R01, t01, mask01 = cv2.recoverPose(E01, point0, point1, K)\n",
    "#_, R12, t12, mask12 = cv2.recoverPose(E12, point1, point2, K)\n",
    "_, R01, t01, mask01 = cv2.recoverPose(E01, point0[matches01[:, 0], :], point1[matches01[:, 1], :], K)\n",
    "_, R12, t12, mask12 = cv2.recoverPose(E12, point1[matches12[:, 0], :], point2[matches12[:, 1], :], K)\n",
    "\n",
    "len(np.where(mask01>0)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The mask returned by cv2.recoverPose indicates which of the matches/**POINTS** are inliers, that lie in front \\\n",
    "of both cameras. Remove the matches that are not inliers from matches01, so that only contains \\\n",
    "the inliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1999, 563)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#matches01_filtered = matches01.copy()\n",
    "#matches12_filtered = matches12.copy()\n",
    "matches01_filtered = matches01[np.where(Emask01>0)[0]]\n",
    "matches12_filtered = matches12[np.where(Emask12>0)[0]]\n",
    "len(matches01), len(matches01_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "## Let's test the filtered matches, by looking at the filtered points on the images ##\n",
    "######################################################################################\n",
    "\n",
    "#im0, im1, im2 = images[0], images[1], images[2]\n",
    "#fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(14, 8))\n",
    "#axs[0, 0].imshow(im0, cmap=\"gray\")\n",
    "#axs[0, 0].scatter(point0[matches01_filtered[:, 0], 0], point0[matches01_filtered[:, 0], 1], s=2, c=\"r\")\n",
    "#axs[0, 0].set_title(\"Image0\")\n",
    "#axs[0, 1].imshow(im1, cmap=\"gray\")\n",
    "#axs[0, 1].scatter(point1[matches01_filtered[:, 1], 0], point1[matches01_filtered[:, 1], 1], s=2, c=\"r\")\n",
    "#axs[0, 1].set_title(\"Image1\")\n",
    "#axs[1, 0].imshow(im1, cmap=\"gray\")\n",
    "#axs[1, 0].scatter(point1[matches12_filtered[:, 0], 0], point1[matches12_filtered[:, 0], 1], s=2, c=\"r\")\n",
    "#axs[1, 0].set_title(\"Image1\")\n",
    "#axs[1, 1].imshow(im2, cmap=\"gray\")\n",
    "#axs[1, 1].scatter(point2[matches12_filtered[:, 1], 0], point2[matches12_filtered[:, 1], 1], s=2, c=\"r\")\n",
    "#axs[1, 1].set_title(\"Image2\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***NB!*** Using KNN ratio test to filter matches produced better results than using the recoverPose output. BUT! Fewer points - and we are doing RANSAC, so more is merrier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.3 Track across all images\n",
    "\n",
    "Use matches01 and matches12 and find the subset of matches such that we can match features \\\n",
    "all the way from image 0 to image 2. In other words, create three lists such that points0[i], \\\n",
    "points1[i], and points2[i] are the 2D locations of the same point in the corresponding images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the intersection of two arrays.\n",
    "# So, in this case where will matches between two first images, continue into the third\n",
    "_, idx01, idx12 = np.intersect1d(matches01_filtered[:,1], matches12_filtered[:,0], return_indices=True)\n",
    "\n",
    "#Return the sorted, unique values that are in both of the input arrays.\n",
    "# So, idx01: indexes of matches in matches01_filtered that were also matches in image 2.as_integer_ratio\n",
    "# And idx12 is also the valid matches in matches12_filtered "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Question*** In the exercise it was only explained to filter matches 01, not matches 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the valid matches - that go throughout the images\n",
    "validmatch01 = matches01_filtered[idx01, :]\n",
    "validmatch12 = matches12_filtered[idx12, :]\n",
    "\n",
    "# Extract the points belonging to the valid matches\n",
    "val0_idx = validmatch01[:, 0]\n",
    "val1_idx = validmatch01[:, 1]\n",
    "val2_idx = validmatch12[:, 1]\n",
    "\n",
    "trans0 = point0[val0_idx, :]\n",
    "trans1 = point1[val1_idx, :]\n",
    "trans2 = point2[val2_idx, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at some images\n",
    "#im0, im1, im2 = images[0], images[1], images[2]\n",
    "#fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(16, 16))\n",
    "#axs[0].imshow(im0, cmap=\"gray\")\n",
    "#axs[0].scatter(trans0[:, 0], trans0[:, 1], s=3, c=\"r\")\n",
    "#axs[0].set_title(\"img0\")\n",
    "#axs[1].imshow(im1, cmap=\"gray\")\n",
    "#axs[1].scatter(trans1[:, 0], trans1[:, 1], s=3, c=\"r\")\n",
    "#axs[1].set_title(\"img1\")\n",
    "#axs[2].imshow(im2, cmap=\"gray\")\n",
    "#axs[2].scatter(trans2[:, 0], trans2[:, 1], s=3, c=\"r\")\n",
    "#axs[2].set_title(\"img2\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.4 Triangulate\n",
    "\n",
    "For the points that have been tracked through all three images, use the 2D positions in image 0\\\n",
    "and 1 to triangulate the points in 3D (Q). Using the 2D positions in image 2, estimate the pose\\\n",
    "of image 2 with RANSAC. Use cv2.solvePnPRansac to do this. As the lens distortion is already\\\n",
    "corrected, you can set ```distCoeffs=np.zeros(5)```.\n",
    "\n",
    "Visualize the 3D points that are also inliers for solvePnPRansac.\n",
    "```\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection=’3d’)\n",
    "ax.scatter(*Q[inliers.flatten()])\n",
    "```\n",
    "Also plot the position of the cameras. Recall that the position of the camera is not the translation.\n",
    "How do you find the position?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "distCoeffs=np.zeros(5) \n",
    "# Projection matrix using \n",
    "P0 = K@np.hstack((np.eye(3), np.zeros((3,1))))\n",
    "P1 = K@np.hstack((R01, t01))\n",
    "P2 = K@np.hstack((R12, t12)) # TODO: Should this not be actual position, or even total pose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.hstack((np.eye(3), np.zeros((3,1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trans3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m trans0 \u001b[38;5;241m=\u001b[39m trans0\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint16)\n\u001b[0;32m      2\u001b[0m trans2 \u001b[38;5;241m=\u001b[39m trans2\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint16)\n\u001b[1;32m----> 3\u001b[0m trans3 \u001b[38;5;241m=\u001b[39m \u001b[43mtrans3\u001b[49m\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint16)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trans3' is not defined"
     ]
    }
   ],
   "source": [
    "trans0 = trans0.astype(np.int16)\n",
    "trans1 = trans1.astype(np.int16)\n",
    "trans2 = trans2.astype(np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triang01 = cv2.triangulatePoints(P0, P1, trans0.T, trans1.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homogenous -> Non-homogenous\n",
    "#triang01 /= triang01[3]\n",
    "#triang01 = triang01[:3]\n",
    "#triang01 = triang01.astype(np.int32)\n",
    "#triang01.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(trans0[0, 0]), type(triang01[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get POSE of camera (not position)\n",
    "#_, rvec, tvec, inliers = cv2.solvePnPRansac(triang01.T, trans2, K, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d1902db9bb2e389c5c5f64e693209aef1412f369d132ca57a092e79ab8be655e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
